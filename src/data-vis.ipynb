{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading repos...\n",
      "******************************\n",
      "Loading preprocessed merged repositories...\n",
      "******************************\n",
      "Total repositories loaded: 31561\n",
      "Loading preprocessed input data...\n",
      "******************************\n",
      "Train on 31561 samples, validate on 31561 samples\n",
      "Epoch 1/10\n",
      "31561/31561 [==============================] - 340s - loss: 0.0913 - val_loss: 0.0805\n",
      "Epoch 2/10\n",
      "31561/31561 [==============================] - 336s - loss: 0.1000 - val_loss: 0.1015\n",
      "Epoch 3/10\n",
      "31561/31561 [==============================] - 338s - loss: 0.1197 - val_loss: 0.1245\n",
      "Epoch 4/10\n",
      "31561/31561 [==============================] - 351s - loss: 0.1286 - val_loss: 0.1250\n",
      "Epoch 5/10\n",
      "31561/31561 [==============================] - 353s - loss: 0.1318 - val_loss: 0.1290\n",
      "Epoch 6/10\n",
      "31561/31561 [==============================] - 356s - loss: 0.1318 - val_loss: 0.1323\n",
      "Epoch 7/10\n",
      "31561/31561 [==============================] - 321s - loss: 0.1321 - val_loss: 0.1345\n",
      "Epoch 8/10\n",
      "31561/31561 [==============================] - 320s - loss: 0.1353 - val_loss: 0.1335\n",
      "Epoch 9/10\n",
      "31561/31561 [==============================] - 317s - loss: 0.1329 - val_loss: 0.1329\n",
      "Epoch 10/10\n",
      "31561/31561 [==============================] - 318s - loss: 0.1320 - val_loss: 0.1338\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "import string\n",
    "\n",
    "import itertools\n",
    "\n",
    "from github3.repos.repo import Repository\n",
    "from github3.users import User\n",
    "\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, merge, Convolution2D, MaxPooling2D, UpSampling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as K\n",
    "\n",
    "metaKeys = [\"stargazers\", \"contributors\", \"subscribers\", \"forks\", \"teams\"]\n",
    "metaDictKeys = {\"stargazers\":\"login\", \"contributors\":\"login\", \"subscribers\":\"login\", \"forks\":\"full_name\", \"teams\":\"full_name\"}\n",
    "DOWNLOAD_FOLDER = \"dl\"\n",
    "FILES = [\"README.md\", \"README\", \"readme.md\", \"readme\"]\n",
    "punctuation_translate_table = dict((ord(char), None) for char in string.punctuation)\n",
    "\n",
    "DESC_ARRAY_SIZE = 256\n",
    "README_ARRAY_SIZE = 512\n",
    "USER_ARRAY_SIZE = 256\n",
    "\n",
    "STARGAZER_WEIGHT, CONTRIBUTOR_WEIGHT, SUBSCRIBER_WEIGHT = 1, 2, 3\n",
    "\n",
    "# Input\n",
    "repos = []\n",
    "repoMeta = []\n",
    "# Processed\n",
    "freqDescription = None\n",
    "freqReadme = None\n",
    "userIndexes = {}\n",
    "repoStargazers, repoContributors, repoSubscribers = [], [], []\n",
    "# Merged users\n",
    "repoUsers = []\n",
    "\n",
    "# ANN\n",
    "inputData = None\n",
    "model = None\n",
    "\n",
    "def has_mask(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def has_mask_loss(y_true, y_pred):\n",
    "    return -has_mask(y_true, y_pred)\n",
    "\n",
    "\n",
    "def createKerasModel():\n",
    "    inputs = Input((1, 100, 2000))\n",
    "\n",
    "    x = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(inputs)\n",
    "    x = MaxPooling2D((2, 2), border_mode='same')(x)\n",
    "    x = Convolution2D(64, 3, 3, activation='relu', border_mode='same')(x)\n",
    "    x = MaxPooling2D((2, 2), border_mode='same')(x)\n",
    "    x = Convolution2D(128, 3, 3, activation='relu', border_mode='same')(x)\n",
    "    encoded = MaxPooling2D((2, 2), border_mode='same')(x)\n",
    "\n",
    "    x = Convolution2D(128, 3, 3, activation='relu', border_mode='same')(encoded)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Convolution2D(64, 3, 3, activation='relu', border_mode='same')(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    decoded = Convolution2D(1, 1, 1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(input=inputs, output=decoded)\n",
    "\n",
    "    #model.compile(optimizer=Adam(lr=1e-5), loss=dice_coef_loss, metrics=[dice_coef])\n",
    "    model.compile(optimizer=Adam(lr=1e-5), loss=has_mask_loss, metrics=[has_mask])\n",
    "\n",
    "\n",
    "'''This script demonstrates how to build a variational autoencoder with Keras.\n",
    "Reference: \"Auto-Encoding Variational Bayes\" https://arxiv.org/abs/1312.6114\n",
    "'''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import savefig\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda, Dropout\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import objectives\n",
    "from keras.datasets import mnist\n",
    "\n",
    "batch_size = 1\n",
    "original_dim = DESC_ARRAY_SIZE + README_ARRAY_SIZE + USER_ARRAY_SIZE\n",
    "latent_dim = 2\n",
    "epsilon_std = 0.01\n",
    "nb_epoch = 10\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_std = args\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim),\n",
    "                              mean=0., std=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_std) * epsilon\n",
    "\n",
    "class MyKerasModel:\n",
    "    def vae_loss(self, x, x_decoded_mean):\n",
    "        xent_loss = objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "        kl_loss = - 0.5 * K.mean(1 + self.z_log_std - K.square(self.z_mean) - K.exp(self.z_log_std), axis=-1) \n",
    "        return xent_loss + kl_loss\n",
    "\n",
    "    def __init__(self):\n",
    "        # Encoder\n",
    "        x = Input(batch_shape=(batch_size, original_dim))\n",
    "        h = Dense(512, activation='relu')(x)\n",
    "        h = Dropout(0.5)(h)\n",
    "        h = Dense(256, activation='relu')(h)\n",
    "        h = Dropout(0.5)(h)\n",
    "        h = Dense(128, activation='relu')(h)\n",
    "        h = Dropout(0.5)(h)\n",
    "        \n",
    "        # Two dimensional latent model\n",
    "        self.z_mean = Dense(2)(h)\n",
    "        self.z_log_std = Dense(2)(h)\n",
    "\n",
    "        # Generator\n",
    "        z = Lambda(sampling)([self.z_mean, self.z_log_std])\n",
    "\n",
    "        # Decoder\n",
    "        decoder_h = Dense(128, activation='relu')(z)\n",
    "        decoder_h = Dropout(0.5)(decoder_h)\n",
    "        decoder_h = Dense(256, activation='relu')(decoder_h)\n",
    "        decoder_h = Dropout(0.5)(decoder_h)\n",
    "        decoder_h = Dense(512, activation='relu')(decoder_h)\n",
    "        decoder_mean = Dense(original_dim, activation='sigmoid')(decoder_h)\n",
    "        #h_decoded = decoder_h(decoder_h_)\n",
    "        #x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "        # VAE model creation\n",
    "        self.vae = Model(x, decoder_mean)\n",
    "        self.vae.compile(optimizer='rmsprop', loss=lambda x, x_decoded_mean: self.vae_loss(x, x_decoded_mean))\n",
    "\n",
    "        # Encoder model creation\n",
    "        self.encoder = Model(x, self.z_mean)\n",
    "\n",
    "        # Generator that can sample from the learned distribution\n",
    "        # and create new repositories\n",
    "        #decoder_input = Input(shape=(latent_dim,))\n",
    "        #_h_decoded = decoder_h(decoder_input)\n",
    "        #_x_decoded_mean = decoder_mean(_h_decoded)\n",
    "        #self.generator = Model(decoder_input, _x_decoded_mean)\n",
    "\n",
    "def executeMNIST():\n",
    "    model = MyKerasModel()\n",
    "\n",
    "    # train the VAE on MNIST digits\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "    x_train = x_train.astype('float32') / 255.\n",
    "    x_test = x_test.astype('float32') / 255.\n",
    "    x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "    x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "\n",
    "    model.vae.fit(x_train, x_train,\n",
    "            shuffle=True,\n",
    "            nb_epoch=nb_epoch,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(x_test, x_test))\n",
    "\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    x_test_encoded = model.encoder.predict(x_test, batch_size=batch_size)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    savefig(\"plot1.png\")\n",
    "    print(\"PLOT1\")\n",
    "\n",
    "    # display a 2D manifold of the digits\n",
    "    n = 15  # figure with 15x15 digits\n",
    "    digit_size = 28\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    # we will sample n points within [-15, 15] standard deviations\n",
    "    grid_x = np.linspace(-15, 15, n)\n",
    "    grid_y = np.linspace(-15, 15, n)\n",
    "\n",
    "    for i, yi in enumerate(grid_x):\n",
    "        for j, xi in enumerate(grid_y):\n",
    "            z_sample = np.array([[xi, yi]]) * epsilon_std\n",
    "            x_decoded = model.generator.predict(z_sample)\n",
    "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "            figure[i * digit_size: (i + 1) * digit_size,\n",
    "                j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(figure)\n",
    "    plt.show()\n",
    "    savefig(\"plot2.png\")\n",
    "    print(\"PLOT2\")\n",
    "\n",
    "#executeMNIST()\n",
    "\n",
    "def loadRepos():\n",
    "    index = 0\n",
    "    while True:\n",
    "        fileName = os.path.join(DOWNLOAD_FOLDER, 'repos' + str(index) + '.pkl')\n",
    "        if not os.path.isfile(fileName):\n",
    "            break\n",
    "        print(\"Loading file: %s...\" % fileName)\n",
    "        for pickled in pickle.load(open(fileName, 'rb')):\n",
    "            repo = Repository.from_dict(pickled[\"repo\"])\n",
    "            repos.append(repo)\n",
    "            meta = pickled[\"meta\"]\n",
    "            repo.forks = [fork[\"full_name\"] for fork in pickled[\"meta\"][\"forks\"]]\n",
    "\n",
    "            # reduce the meta dicts by only using the value of the metaDictKey\n",
    "            for key in metaKeys:\n",
    "                metaDictKey = metaDictKeys[key]\n",
    "                valueList = meta.get(key) or []\n",
    "                meta[key] = [value[metaDictKey] for value in valueList]\n",
    "            meta[\"README\"] = \"\"\n",
    "            for FILE in FILES:\n",
    "                if FILE in meta[\"files\"]:\n",
    "                    meta[\"README\"] = meta[\"files\"][FILE].decode(encoding='UTF-8')\n",
    "                    meta[\"files\"] = None\n",
    "                    break\n",
    "\n",
    "            repoMeta.append(meta)\n",
    "            #for fork in repo.forks:\n",
    "                #print(fork, pickled[\"repo\"][\"full_name\"], Repository.from_dict(pickled[\"repo\"]).full_name)\n",
    "\n",
    "        index += 1\n",
    "        #break\n",
    "    return repos, repoMeta\n",
    "\n",
    "def mergeRepos(repo1, meta1, repo2, meta2):\n",
    "    for key in metaKeys:\n",
    "        meta1[key] = set(meta1[key]).union(set(meta2[key]))\n",
    "    if meta1[\"README\"] == \"\":\n",
    "        meta1[\"README\"] = meta2[\"README\"]\n",
    "\n",
    "def forkMerge():\n",
    "    global repos, repoMeta\n",
    "    forkNameIndex = {}\n",
    "    synsets = []\n",
    "    print(\"Fork merge...\")\n",
    "    print(\"*\"*30)\n",
    "    print(\"Preparing synonym sets...\")\n",
    "    for i, repo in enumerate(repos):\n",
    "        forkNameIndex[repo.full_name] = i\n",
    "        synsets.append([])\n",
    "    for i, repo in enumerate(repos):\n",
    "        for fork in repo.forks:\n",
    "            index = forkNameIndex.get(fork)\n",
    "            if index is not None:\n",
    "                synsets[i].append(index)\n",
    "                synsets[index].append(i)\n",
    "    print(\"Merging synonym sets...\")\n",
    "    mergedRepos = []\n",
    "    mergedRepoMeta = []\n",
    "    for i, synset in enumerate(synsets):\n",
    "        # check if already merged\n",
    "        if len(synsets[i]) != 0 and min(synsets[i]) < i:\n",
    "            continue\n",
    "        repo1, meta1 = repos[i], repoMeta[i]\n",
    "        for index in synset:\n",
    "            repo2, meta2 = repos[index], repoMeta[i]\n",
    "            mergeRepos(repo1, meta1, repo2, meta2)\n",
    "        mergedRepos.append(repo1)\n",
    "        mergedRepoMeta.append(meta1)\n",
    "    print(\"Repositories before merge: %d\" % len(repos))\n",
    "    print(\"Repositories after merge: %d\" % len(mergedRepos))\n",
    "    repos = mergedRepos\n",
    "    repoMeta = mergedRepoMeta\n",
    "    # cache to file\n",
    "    output = {\"repos\":[{\"full_name\":repo.full_name, \"description\":repo.description} for repo in repos], \"repoMeta\":repoMeta}\n",
    "    pickle.dump(output, open(\"merged_repos.pkl\", \"wb\"))\n",
    "\n",
    "def preprocess():\n",
    "    loadRepos()\n",
    "\n",
    "def processText(text):\n",
    "    lowers = text.lower()\n",
    "    no_punctuation = lowers.translate(punctuation_translate_table)\n",
    "    tokens = nltk.word_tokenize(no_punctuation)\n",
    "    filtered = [w for w in tokens if not w in stopwords.words('english')]\n",
    "    return filtered\n",
    "\n",
    "def tokens2Array(tokens, arraySize, key, startIndex):\n",
    "    counter = Counter(tokens)\n",
    "    common = sorted(counter.items(), key=lambda pair: pair[1], reverse=True)\n",
    "    common = common[:arraySize]\n",
    "\n",
    "    commonKeys = {}\n",
    "    for i, item in enumerate(common):\n",
    "        commonKeys[item[0]] = i\n",
    "\n",
    "    for i, repo in enumerate(repos):\n",
    "        if key == \"users\":\n",
    "            for user in repoStargazers[i]:\n",
    "                tokenIndex = commonKeys.get(user)\n",
    "                if tokenIndex:\n",
    "                    inputData[i, tokenIndex+startIndex] += STARGAZER_WEIGHT\n",
    "            for user in repoContributors[i]:\n",
    "                tokenIndex = commonKeys.get(user)\n",
    "                if tokenIndex:\n",
    "                    inputData[i, tokenIndex+startIndex] += CONTRIBUTOR_WEIGHT\n",
    "            for user in repoSubscribers[i]:\n",
    "                tokenIndex = commonKeys.get(user)\n",
    "                if tokenIndex:\n",
    "                    inputData[i, tokenIndex+startIndex] += SUBSCRIBER_WEIGHT\n",
    "        else:\n",
    "            if key == \"description\": #HACK\n",
    "                text = repo.get(key)\n",
    "            else:\n",
    "                text = repoMeta[i].get(key)\n",
    "            if text is not None and text != \"\":\n",
    "                tokens = processText(text)\n",
    "                tokens = list(set(tokens))\n",
    "                for token in tokens:\n",
    "                    tokenIndex = commonKeys.get(token)\n",
    "                    if tokenIndex:\n",
    "                        inputData[i, tokenIndex+startIndex] = 1\n",
    "\n",
    "    return common, commonKeys\n",
    "\n",
    "def processDescription():\n",
    "    global freqDescription\n",
    "    allTextTokens = []\n",
    "    hasDesc = 0\n",
    "    for repo in repos:\n",
    "        if repo[\"description\"] is not None:\n",
    "            if repo[\"description\"] != \"\":\n",
    "                hasDesc += 1\n",
    "            tokens = processText(repo[\"description\"])\n",
    "            tokens = list(set(tokens))\n",
    "            allTextTokens += tokens\n",
    "    print(\"Total description files: %d, in %.2f%% repositories\" % (hasDesc, 100*hasDesc / len(repos)))\n",
    "    return tokens2Array(allTextTokens, DESC_ARRAY_SIZE, \"description\", 0)\n",
    "\n",
    "def processReadmes():\n",
    "    global freqReadme\n",
    "    allTextTokens = []\n",
    "    hasRepo = 0\n",
    "    for meta in repoMeta:\n",
    "        if meta[\"README\"] != \"\":\n",
    "            hasRepo += 1\n",
    "        tokens = processText(meta[\"README\"])\n",
    "        tokens = list(set(tokens))\n",
    "        allTextTokens += tokens\n",
    "    print(\"Total readme files: %d, in %.2f%% repositories\" % (hasRepo, 100*hasRepo / len(repoMeta)))\n",
    "    return tokens2Array(allTextTokens, README_ARRAY_SIZE, \"README\", DESC_ARRAY_SIZE)\n",
    "\n",
    "def addUser(repoIndx, user, mapping):\n",
    "    if not user in userIndexes:\n",
    "        userIndx = len(userIndexes) + 1\n",
    "        userIndexes[user] = userIndx\n",
    "        mapping[repoIndx].append(userIndx)\n",
    "\n",
    "        #for v in maps.values():\n",
    "        #mapping[username] = 1\n",
    "    #else:\n",
    "        #mapping[username] += 1\n",
    "\n",
    "def processUsers():\n",
    "    print(\"Processing users...\")\n",
    "    print(\"*\"*30)\n",
    "\n",
    "    users = []\n",
    "    for i, meta in enumerate(repoMeta):\n",
    "        #repoUsers.append([])\n",
    "        repoStargazers.append([])\n",
    "        repoContributors.append([])\n",
    "        repoSubscribers.append([])\n",
    "        for user in meta[\"stargazers\"]:\n",
    "            addUser(i, user, repoStargazers)\n",
    "        for user in meta[\"contributors\"]:\n",
    "            addUser(i, user, repoContributors)\n",
    "        for user in meta[\"subscribers\"]:\n",
    "            addUser(i, user, repoSubscribers)\n",
    "        users += list(set(repoStargazers[i] + repoContributors[i] + repoSubscribers[i]))\n",
    "        sys.stdout.write(\"\\rProcessed %d/%d repositories.\" % (i, len(repoMeta)))\n",
    "    print()\n",
    "    print(\"Total users: %d\" % len(userIndexes))\n",
    "    allSubscribers = list(itertools.chain.from_iterable(repoSubscribers))\n",
    "    allStargazers = list(itertools.chain.from_iterable(repoStargazers))\n",
    "    allContributors = list(itertools.chain.from_iterable(repoContributors))\n",
    "\n",
    "    print(\"Stargazing: %d, contributing: %d, subscribing: %d\" % (len(allStargazers),len(allContributors), len(allSubscribers)))\n",
    "\n",
    "    print(\"Repo average Stargazing: %.4f, contributing: %.4f, subscribing: %.4f\" % (len(allStargazers)/len(repos),len(allContributors)/len(repos), len(allSubscribers)/len(repos)))\n",
    "\n",
    "    return tokens2Array(users, USER_ARRAY_SIZE, \"users\", DESC_ARRAY_SIZE + README_ARRAY_SIZE)\n",
    "    #pickle.dump({\"userIndexes\":userIndexes, \"repoStargazers\":repoStargazers, \"repoContributors\":repoContributors, \"repoSubscribers\":repoSubscribers}, open(\"user_indexes.pkl\", 'wb'))\n",
    "    #\n",
    "    #sum(usersToOccurance.values())/len(usersToOccurance))\n",
    "\n",
    "def plot():\n",
    "    import matplotlib.cm as cmx\n",
    "import matplotlib.colors as clrs\n",
    "\n",
    "def get_cmap(N):\n",
    "    '''Returns a function that maps each index in 0, 1, ... N-1 to a distinct \n",
    "    RGB color.'''\n",
    "    color_norm  = clrs.Normalize(vmin=0, vmax=N-1)\n",
    "    scalar_map = cmx.ScalarMappable(norm=color_norm, cmap='hsv') \n",
    "    def map_index_to_rgb_color(index):\n",
    "        return scalar_map.to_rgba(index)\n",
    "    return map_index_to_rgb_color\n",
    "\n",
    "\n",
    "    import matplotlib.patches as mpatches\n",
    "\n",
    "    #print(readme)\n",
    "    #readmeKeys['ruby']\n",
    "    #readmeKeys['rails']\n",
    "    #readmeKeys['gem']\n",
    "    #readmeKeys['javascript']\n",
    "    #readmeKeys['c']\n",
    "    #readmeKeys['python']\n",
    "    colorKeys = {}\n",
    "    colorKeys['ruby'] = 1\n",
    "    colorKeys['rails'] = 1\n",
    "    colorKeys['gem'] = 1\n",
    "    colorKeys['python'] = 2\n",
    "    colorKeys['django'] = 2\n",
    "    colorKeys['javascript'] = 3\n",
    "    colorKeys['jquery'] = 3\n",
    "    colorKeys['php'] = 4\n",
    "    colorKeys['perl'] = 5\n",
    "    colorKeys['c'] = 6 \n",
    "    colorKeys['html'] = 7\n",
    "    colorKeys['erlang'] = 8\n",
    "\n",
    "\n",
    "    colors = np.zeros(inputData.shape[0])\n",
    "    for i, data in enumerate(inputData):\n",
    "        for colorKey in colorKeys.keys():\n",
    "            index = descriptionKeys[colorKey]\n",
    "            if inputData[i, index] == 1:\n",
    "                colors[i] = colorKeys[colorKey]\n",
    "                break\n",
    "    print()\n",
    "    plotData = inputData[colors!=0]\n",
    "    colors = colors[colors!=0]\n",
    "    colors = cmap(colors)\n",
    "\n",
    "    colorKeys['rails'] = None\n",
    "    colorKeys['gem'] = None\n",
    "    colorKeys['django'] = None\n",
    "    colorKeys['jquery'] = None\n",
    "    invColorKeys = {v: k for k, v in colorKeys.items()}\n",
    "    labels = [invColorKeys[i] for i in range(1, 9)]\n",
    "    cmap = get_cmap(len(invColorKeys))\n",
    "\n",
    "    # display a 2D plot of the repository classes in the latent space\n",
    "    #x_test_encoded = model.encoder.predict(inputData, batch_size=batch_size)\n",
    "    x_test_encoded = model.encoder.predict(plotData, batch_size=batch_size)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    scatter = plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=colors)\n",
    "\n",
    "    recs = []\n",
    "    for i in range(1,len(invColorKeys)+1):\n",
    "        recs.append(mpatches.Rectangle((0,0),1,1,fc=cmap(i)))\n",
    "    plt.legend(recs,labels,loc=4)\n",
    "\n",
    "    #plt.legend(scatter,\n",
    "    #           [invColorKeys[i] for i in range(1, 9)],\n",
    "    #           scatterpoints=1,\n",
    "    #           loc='lower left',\n",
    "    #           ncol=3,\n",
    "    #           fontsize=8)\n",
    "\n",
    "    plt.show()\n",
    "    savefig(\"plot1-repo.png\")\n",
    "    print(\"Plotted repository map\")\n",
    "\n",
    "\n",
    "def execute():\n",
    "    global repos, repoMeta, inputData, model\n",
    "    print(\"Loading repos...\")\n",
    "    print(\"*\"*30)\n",
    "    try:\n",
    "        print(\"Loading preprocessed merged repositories...\")\n",
    "        print(\"*\"*30)\n",
    "        pickled = pickle.load(open(\"merged_repos.pkl\", \"rb\"))\n",
    "        repos, repoMeta = pickled[\"repos\"], pickled[\"repoMeta\"]\n",
    "    except Exception as x:\n",
    "        print(x)\n",
    "        traceback.print_exc()\n",
    "        preprocess()\n",
    "        forkMerge()\n",
    "    print(\"Total repositories loaded: %d\" % len(repos))\n",
    "\n",
    "    try:\n",
    "        print(\"Loading preprocessed input data...\")\n",
    "        print(\"*\"*30)\n",
    "        inputData = pickle.load(open(\"input_data_nn.pkl\", \"rb\"))\n",
    "        #print(\"Loading preprocessed user indexes repositories...\")\n",
    "        #print(\"*\"*30)\n",
    "        #pickled = pickle.load(open(\"user_indexes.pkl\", 'rb'))\n",
    "        #userIndexes = pickled[\"userIndexes\"]\n",
    "        #repoUsers = pickled[\"repoUsers\"]\n",
    "\n",
    "        #print(userIndexes, repoUsers)\n",
    "    except Exception as x:\n",
    "        print(x)\n",
    "        traceback.print_exc()\n",
    "\n",
    "        inputData = np.zeros((len(repos), original_dim))\n",
    "        users, usersKeys = processUsers()\n",
    "        description, descriptionKeys = processDescription()\n",
    "        readme, readmeKeys = processReadmes()\n",
    "        pickle.dump({\"description\":description, \"descriptionKeys\":descriptionKeys, \"readme\":readme, \"readmeKeys\":readmeKeys, \"users\":users, \"usersKeys\":usersKeys}, \n",
    "                    open(\"input_data_processed.pkl\", \"wb\"))\n",
    "\n",
    "        pickle.dump(inputData, open(\"input_data_nn.pkl\", \"wb\"))\n",
    "\n",
    "    model = MyKerasModel()\n",
    "    model.vae.fit(inputData, inputData,\n",
    "            shuffle=True,\n",
    "            nb_epoch=nb_epoch,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(inputData, inputData))\n",
    "\n",
    "execute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/big/work/somet-2016/env/lib/python3.5/site-packages/matplotlib/pyplot.py:516: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotted repository map\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.cm as cmx\n",
    "import matplotlib.colors as clrs\n",
    "\n",
    "def get_cmap(N):\n",
    "    '''Returns a function that maps each index in 0, 1, ... N-1 to a distinct \n",
    "    RGB color.'''\n",
    "    color_norm  = clrs.Normalize(vmin=0, vmax=N-1)\n",
    "    scalar_map = cmx.ScalarMappable(norm=color_norm, cmap='hsv') \n",
    "    def map_index_to_rgb_color(index):\n",
    "        return scalar_map.to_rgba(index)\n",
    "    return map_index_to_rgb_color\n",
    "\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "#print(readme)\n",
    "#readmeKeys['ruby']\n",
    "#readmeKeys['rails']\n",
    "#readmeKeys['gem']\n",
    "#readmeKeys['javascript']\n",
    "#readmeKeys['c']\n",
    "#readmeKeys['python']\n",
    "colorKeys = {}\n",
    "colorKeys['ruby'] = 1\n",
    "colorKeys['rails'] = 1\n",
    "colorKeys['gem'] = 1\n",
    "colorKeys['python'] = 2\n",
    "colorKeys['django'] = 2\n",
    "colorKeys['javascript'] = 3\n",
    "colorKeys['jquery'] = 3\n",
    "colorKeys['php'] = 4\n",
    "colorKeys['perl'] = 5\n",
    "colorKeys['c'] = 6 \n",
    "colorKeys['html'] = 7\n",
    "colorKeys['erlang'] = 8\n",
    "\n",
    "\n",
    "colors = np.zeros(inputData.shape[0])\n",
    "for i, data in enumerate(inputData):\n",
    "    for colorKey in colorKeys.keys():\n",
    "        index = descriptionKeys[colorKey]\n",
    "        if inputData[i, index] == 1:\n",
    "            colors[i] = colorKeys[colorKey]\n",
    "            break\n",
    "print()\n",
    "plotData = inputData[colors!=0]\n",
    "colors = colors[colors!=0]\n",
    "colors = cmap(colors)\n",
    "\n",
    "colorKeys['rails'] = None\n",
    "colorKeys['gem'] = None\n",
    "colorKeys['django'] = None\n",
    "colorKeys['jquery'] = None\n",
    "invColorKeys = {v: k for k, v in colorKeys.items()}\n",
    "labels = [invColorKeys[i] for i in range(1, 9)]\n",
    "cmap = get_cmap(len(invColorKeys))\n",
    "\n",
    "# display a 2D plot of the repository classes in the latent space\n",
    "#x_test_encoded = model.encoder.predict(inputData, batch_size=batch_size)\n",
    "x_test_encoded = model.encoder.predict(plotData, batch_size=batch_size)\n",
    "plt.figure(figsize=(6, 6))\n",
    "scatter = plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=colors)\n",
    "\n",
    "recs = []\n",
    "for i in range(1,len(invColorKeys)+1):\n",
    "    recs.append(mpatches.Rectangle((0,0),1,1,fc=cmap(i)))\n",
    "plt.legend(recs,labels,loc=4)\n",
    "\n",
    "#plt.legend(scatter,\n",
    "#           [invColorKeys[i] for i in range(1, 9)],\n",
    "#           scatterpoints=1,\n",
    "#           loc='lower left',\n",
    "#           ncol=3,\n",
    "#           fontsize=8)\n",
    "\n",
    "plt.show()\n",
    "savefig(\"plot1-repo.png\")\n",
    "print(\"Plotted repository map\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f89a76e9e80>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n",
      "(60000,)\n",
      "(10000,)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'description' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-1f23cc1973c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m pickle.dump({\"description\":description, \"descriptionKeys\":descriptionKeys, \"readme\":readme, \"readmeKeys\":readmeKeys, \"users\":users, \"usersKeys\":usersKeys}, \n\u001b[0m\u001b[0;32m      7\u001b[0m                     open(\"input_data_processed.pkl\", \"wb\"))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'description' is not defined"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "pickle.dump({\"description\":description, \"descriptionKeys\":descriptionKeys, \"readme\":readme, \"readmeKeys\":readmeKeys, \"users\":users, \"usersKeys\":usersKeys}, \n",
    "                    open(\"input_data_processed.pkl\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SOMET2016-Python 3",
   "language": "python",
   "name": "somet-2016-python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
